# Database Production Reliability Roadmap
## Enigma QR Menu Application - PostgreSQL Self-Hosted Supabase

**Document Version:** 1.0
**Last Updated:** 2025-10-01
**Database:** PostgreSQL 15+ (Supabase Self-Hosted)
**Schema:** restaurante (31 tables)
**Connection:** supabase.enigmaconalma.com:8443

---

## Executive Summary

This document provides a comprehensive production reliability strategy for the Enigma QR Menu App database, focusing on error prevention, data integrity, performance optimization, and troubleshooting capabilities. The system handles critical operations including menu management, order processing with atomic stock updates, QR-based table sessions, and GDPR-compliant customer data.

### Current Health Status

**‚úÖ STRENGTHS:**
- Connection pooling: 17/100 connections (17% utilization - healthy)
- Database size: 156 MB (manageable)
- Autovacuum active on critical tables
- Service role key for RLS bypass working correctly
- Atomic stock management via RPC function

**‚ö†Ô∏è CONCERNS:**
- `orders` table has 32 dead tuples (needs vacuum)
- Missing performance indexes for order lookups
- No connection timeout configuration visible
- pg_stat_statements shows DDL migrations in top queries (cleanup needed)
- PostgreSQL memory settings are minimal (16384 shared_buffers = 128MB)

---

## 1. Production Health Checks

### 1.1 Database Connection Monitoring

#### Connection Pool Health Script
```sql
-- /scripts/monitoring/connection-pool-health.sql
-- Run every 5 minutes via cron

SELECT
  count(*) as active_connections,
  max_conn,
  ROUND(100.0 * count(*) / max_conn, 2) as utilization_pct,
  CASE
    WHEN count(*) > max_conn * 0.8 THEN 'üî¥ CRITICAL'
    WHEN count(*) > max_conn * 0.6 THEN '‚ö†Ô∏è WARNING'
    ELSE '‚úÖ HEALTHY'
  END as status
FROM pg_stat_activity
CROSS JOIN (SELECT setting::int as max_conn FROM pg_settings WHERE name = 'max_connections') s
WHERE datname = 'postgres';

-- Detailed breakdown by state
SELECT
  state,
  count(*) as connections,
  ROUND(avg(EXTRACT(EPOCH FROM (now() - state_change))), 2) as avg_duration_sec
FROM pg_stat_activity
WHERE datname = 'postgres'
GROUP BY state
ORDER BY connections DESC;

-- Detect long-running queries (potential leaks)
SELECT
  pid,
  usename,
  application_name,
  state,
  query_start,
  EXTRACT(EPOCH FROM (now() - query_start)) as duration_sec,
  left(query, 80) as query_preview
FROM pg_stat_activity
WHERE datname = 'postgres'
  AND state != 'idle'
  AND query_start < now() - interval '30 seconds'
ORDER BY query_start;
```

**Alert Thresholds:**
- **üî¥ CRITICAL:** >80% connection utilization (>80/100 connections)
- **‚ö†Ô∏è WARNING:** >60% utilization (>60/100 connections)
- **üü° SLOW QUERY:** Any query running >30 seconds
- **üü† IDLE IN TRANSACTION:** Connections idle >5 minutes

#### Connection Leak Detection
```sql
-- Detect potential connection leaks from Next.js API routes
SELECT
  application_name,
  state,
  count(*) as connection_count,
  max(state_change) as last_activity
FROM pg_stat_activity
WHERE datname = 'postgres'
GROUP BY application_name, state
HAVING count(*) > 5  -- More than 5 connections from same app
ORDER BY connection_count DESC;
```

### 1.2 Query Performance Tracking

#### Enable pg_stat_statements (CRITICAL)
```sql
-- Run once on database initialization
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- Add to postgresql.conf (requires restart):
-- shared_preload_libraries = 'pg_stat_statements'
-- pg_stat_statements.track = all
-- pg_stat_statements.max = 10000
```

#### Slow Query Analysis
```sql
-- /scripts/monitoring/slow-queries.sql
-- Top 10 slowest queries by average execution time
SELECT
  ROUND(mean_exec_time::numeric, 2) as avg_ms,
  ROUND(total_exec_time::numeric, 2) as total_ms,
  calls,
  ROUND((100.0 * total_exec_time / sum(total_exec_time) OVER ())::numeric, 2) as pct_total,
  left(query, 120) as query_preview
FROM pg_stat_statements
WHERE query NOT LIKE '%pg_stat_statements%'
  AND query NOT LIKE 'CREATE %'
  AND query NOT LIKE 'ALTER %'
  AND dbid = (SELECT oid FROM pg_database WHERE datname = 'postgres')
ORDER BY mean_exec_time DESC
LIMIT 10;

-- Queries consuming most total time
SELECT
  ROUND(total_exec_time::numeric, 2) as total_ms,
  calls,
  ROUND(mean_exec_time::numeric, 2) as avg_ms,
  ROUND((100.0 * total_exec_time / sum(total_exec_time) OVER ())::numeric, 2) as pct_total,
  left(query, 120) as query_preview
FROM pg_stat_statements
WHERE query NOT LIKE '%pg_stat_statements%'
  AND dbid = (SELECT oid FROM pg_database WHERE datname = 'postgres')
ORDER BY total_exec_time DESC
LIMIT 10;
```

**Performance Targets:**
- Menu fetch (get_complete_menu): <100ms average
- Order creation: <200ms average
- Stock update (decrease_menu_item_stock RPC): <50ms average
- QR scan validation: <100ms average

### 1.3 Index Efficiency Analysis

```sql
-- /scripts/monitoring/index-usage.sql
-- Identify unused indexes (candidates for removal)
SELECT
  schemaname,
  tablename,
  indexname,
  idx_scan as index_scans,
  pg_size_pretty(pg_relation_size(indexrelid)) as index_size
FROM pg_stat_user_indexes
WHERE schemaname = 'restaurante'
  AND idx_scan = 0
  AND indexname NOT LIKE '%_pkey'
ORDER BY pg_relation_size(indexrelid) DESC;

-- Identify missing indexes (tables with high seq_scans)
SELECT
  schemaname,
  relname as table_name,
  seq_scan as sequential_scans,
  seq_tup_read as rows_read,
  idx_scan as index_scans,
  ROUND(100.0 * seq_tup_read / NULLIF(seq_scan, 0), 2) as avg_rows_per_scan,
  pg_size_pretty(pg_relation_size(relid)) as table_size
FROM pg_stat_user_tables
WHERE schemaname = 'restaurante'
  AND seq_scan > 0
ORDER BY seq_tup_read DESC
LIMIT 10;
```

### 1.4 Table Bloat and Vacuum Monitoring

```sql
-- /scripts/monitoring/table-health.sql
SELECT
  schemaname,
  relname as table_name,
  n_live_tup as live_rows,
  n_dead_tup as dead_rows,
  ROUND(100.0 * n_dead_tup / NULLIF(n_live_tup + n_dead_tup, 0), 2) as bloat_pct,
  last_vacuum,
  last_autovacuum,
  CASE
    WHEN n_dead_tup > 1000 AND last_autovacuum IS NULL THEN 'üî¥ NEEDS VACUUM'
    WHEN n_dead_tup::float / NULLIF(n_live_tup, 0) > 0.2 THEN '‚ö†Ô∏è HIGH BLOAT'
    ELSE '‚úÖ HEALTHY'
  END as status,
  pg_size_pretty(pg_total_relation_size(relid)) as total_size
FROM pg_stat_user_tables
WHERE schemaname = 'restaurante'
  AND (n_dead_tup > 100 OR last_autovacuum < now() - interval '7 days')
ORDER BY n_dead_tup DESC;

-- Manual vacuum command for critical tables
-- VACUUM ANALYZE restaurante.orders;
-- VACUUM ANALYZE restaurante.order_items;
-- VACUUM ANALYZE restaurante.menu_items;
```

**Current Issues Detected:**
- ‚úÖ `order_items`: 0 dead tuples (healthy, last autovacuum: 2025-10-01)
- ‚úÖ `menu_items`: 53 dead tuples (healthy, autovacuumed: 2025-09-21)
- ‚ö†Ô∏è `orders`: 32 dead tuples from 4 live rows (80% bloat - VACUUM RECOMMENDED)

---

## 2. Error Prevention

### 2.1 Database-Level Input Validation

#### Enhanced Constraints
```sql
-- /migrations/add-production-constraints.sql

-- Order amount validation
ALTER TABLE restaurante.orders
ADD CONSTRAINT check_total_amount_positive
CHECK (totalAmount >= 0 AND totalAmount < 10000);

-- Party size realistic bounds
ALTER TABLE restaurante.reservations
ADD CONSTRAINT check_realistic_party_size
CHECK (partySize BETWEEN 1 AND 50);

-- Stock cannot go negative (defensive)
ALTER TABLE restaurante.menu_items
ADD CONSTRAINT check_stock_non_negative
CHECK (stock >= 0);

-- Order quantity validation
ALTER TABLE restaurante.order_items
ADD CONSTRAINT check_quantity_positive
CHECK (quantity > 0 AND quantity <= 100);

-- Price validation
ALTER TABLE restaurante.menu_items
ADD CONSTRAINT check_price_positive
CHECK (price > 0 AND price < 1000);

-- Email format validation (basic)
ALTER TABLE restaurante.orders
ADD CONSTRAINT check_email_format
CHECK (customer_email ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$');
```

#### Trigger-Based Validation
```sql
-- Prevent order creation for unavailable menu items
CREATE OR REPLACE FUNCTION restaurante.validate_menu_item_availability()
RETURNS TRIGGER AS $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM restaurante.menu_items
    WHERE id = NEW."menuItemId"
    AND "isAvailable" = true
  ) THEN
    RAISE EXCEPTION 'Menu item % is not available', NEW."menuItemId";
  END IF;
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER check_menu_item_before_order_item
BEFORE INSERT ON restaurante.order_items
FOR EACH ROW
EXECUTE FUNCTION restaurante.validate_menu_item_availability();
```

### 2.2 Constraint Design Review

**Current Constraints Audit:**

‚úÖ **Strong Constraints:**
- `orders_pkey`: Primary key (id)
- `orders_orderNumber_key`: Unique order numbers
- `check_order_source`: Enum validation (presencial/qr/online/telefono)
- `order_items_orderId_fkey`: ON DELETE RESTRICT (prevents orphaned items)
- `menu_items_categoryId_fkey`: ON DELETE RESTRICT (maintains referential integrity)

‚ö†Ô∏è **Missing Constraints:**
- No `CHECK` on totalAmount/unitPrice positivity
- No `CHECK` on quantity bounds
- No `CHECK` on stock negativity prevention
- No `CHECK` on date future validation for reservations

### 2.3 Transaction Isolation Strategy

```sql
-- Current RPC function uses implicit transaction
-- Recommended: Add explicit transaction management for complex operations

CREATE OR REPLACE FUNCTION restaurante.create_order_with_stock_check(
  p_order_data JSONB,
  p_order_items JSONB[]
) RETURNS TABLE(order_id TEXT, success BOOLEAN) AS $$
DECLARE
  v_order_id TEXT;
  v_item JSONB;
BEGIN
  -- Set transaction isolation level
  SET LOCAL TRANSACTION ISOLATION LEVEL SERIALIZABLE;

  -- Create order
  INSERT INTO restaurante.orders (
    id, "orderNumber", "totalAmount", status, "tableId", "restaurantId", "orderedAt", "updatedAt"
  )
  SELECT
    (p_order_data->>'id')::TEXT,
    (p_order_data->>'orderNumber')::TEXT,
    (p_order_data->>'totalAmount')::NUMERIC,
    (p_order_data->>'status')::restaurante."OrderStatus",
    (p_order_data->>'tableId')::TEXT,
    (p_order_data->>'restaurantId')::TEXT,
    CURRENT_TIMESTAMP,
    CURRENT_TIMESTAMP
  RETURNING id INTO v_order_id;

  -- Process each item atomically
  FOREACH v_item IN ARRAY p_order_items LOOP
    -- Check stock availability first
    IF (SELECT stock FROM restaurante.menu_items WHERE id = (v_item->>'menuItemId')::TEXT) < (v_item->>'quantity')::INTEGER THEN
      RAISE EXCEPTION 'Insufficient stock for item %', v_item->>'menuItemId';
    END IF;

    -- Insert order item
    INSERT INTO restaurante.order_items (
      id, "orderId", "menuItemId", quantity, "unitPrice", "totalPrice", status, "createdAt", "updatedAt"
    )
    VALUES (
      (v_item->>'id')::TEXT,
      v_order_id,
      (v_item->>'menuItemId')::TEXT,
      (v_item->>'quantity')::INTEGER,
      (v_item->>'unitPrice')::NUMERIC,
      (v_item->>'totalPrice')::NUMERIC,
      'PENDING'::restaurante."OrderItemStatus",
      CURRENT_TIMESTAMP,
      CURRENT_TIMESTAMP
    );

    -- Decrease stock atomically
    PERFORM restaurante.decrease_menu_item_stock(
      (v_item->>'menuItemId')::TEXT,
      (v_item->>'quantity')::INTEGER
    );
  END LOOP;

  RETURN QUERY SELECT v_order_id, TRUE;

EXCEPTION WHEN OTHERS THEN
  -- Rollback handled automatically
  RAISE EXCEPTION 'Order creation failed: %', SQLERRM;
END;
$$ LANGUAGE plpgsql;
```

### 2.4 Idempotency Patterns

```sql
-- Idempotent order creation using unique constraint
-- Prevents duplicate orders from retry attempts

ALTER TABLE restaurante.orders
ADD COLUMN IF NOT EXISTS idempotency_key TEXT;

CREATE UNIQUE INDEX idx_orders_idempotency_key
ON restaurante.orders(idempotency_key)
WHERE idempotency_key IS NOT NULL;

-- Application layer should generate idempotency key:
-- const idempotencyKey = `order_${tableId}_${timestamp}_${randomString}`
```

**Implementation in API Route:**
```typescript
// /src/app/api/orders/route.ts
const idempotencyKey = request.headers.get('idempotency-key')

// Try insert with ON CONFLICT
const { data, error } = await supabase
  .from('orders')
  .insert({ ...orderData, idempotency_key: idempotencyKey })
  .select()
  .single()

if (error?.code === '23505') {
  // Duplicate idempotency key - return existing order
  const existing = await supabase
    .from('orders')
    .select()
    .eq('idempotency_key', idempotencyKey)
    .single()

  return NextResponse.json({ order: existing.data, duplicate: true })
}
```

---

## 3. Troubleshooting Toolkit

### 3.1 Common Error Scenarios

#### Error: "Stock insuficiente o item no encontrado"
```sql
-- Diagnostic query
SELECT
  id,
  name,
  stock,
  "isAvailable",
  "categoryId"
FROM restaurante.menu_items
WHERE id = $1;  -- Replace with problematic item_id

-- Check recent stock changes
SELECT
  oi."menuItemId",
  mi.name,
  SUM(oi.quantity) as total_ordered,
  mi.stock as current_stock,
  mi.stock + SUM(oi.quantity) as original_stock
FROM restaurante.order_items oi
JOIN restaurante.menu_items mi ON oi."menuItemId" = mi.id
WHERE oi."createdAt" > now() - interval '24 hours'
  AND oi."menuItemId" = $1
GROUP BY oi."menuItemId", mi.name, mi.stock;
```

**Resolution:**
```sql
-- Restore stock if incorrect (requires audit)
UPDATE restaurante.menu_items
SET stock = 50  -- Correct value
WHERE id = 'menu_item_id'
  AND stock < 0;  -- Only fix negative stock
```

#### Error: "Table already reserved"
```sql
-- Diagnostic: Find conflicting reservations
SELECT
  r.id,
  r."customerName",
  r.date,
  r.time,
  r.status,
  r.table_ids,
  r."tableId",
  EXTRACT(EPOCH FROM (r.time - $1::timestamp)) / 60 as minutes_diff
FROM restaurante.reservations r
WHERE r.date::date = $2::date
  AND r.status IN ('PENDING', 'CONFIRMED', 'SEATED')
  AND (
    r.table_ids && $3::text[]  -- Array overlap operator
    OR r."tableId" = ANY($3::text[])
  )
ORDER BY r.time;
```

**Resolution:**
- Check if reservation was cancelled but status not updated
- Verify buffer_minutes configuration is appropriate
- Consider table_ids vs tableId legacy conflict

#### Error: "Failed to create reservation token"
```sql
-- Check token generation
SELECT
  id,
  "reservationId",
  token,
  "expiresAt",
  "isUsed"
FROM restaurante.reservation_tokens
WHERE "reservationId" = $1
ORDER BY "createdAt" DESC;

-- Regenerate token manually
INSERT INTO restaurante.reservation_tokens (
  id,
  "reservationId",
  token,
  "expiresAt",
  "createdAt"
)
VALUES (
  gen_random_uuid()::text,
  $1,  -- reservation_id
  encode(gen_random_bytes(32), 'hex'),
  now() + interval '7 days',
  now()
)
RETURNING token;
```

### 3.2 Data Integrity Verification Scripts

#### Orphaned Records Detection
```sql
-- /scripts/integrity/orphaned-records.sql

-- Order items without valid orders
SELECT oi.id, oi."orderId"
FROM restaurante.order_items oi
LEFT JOIN restaurante.orders o ON oi."orderId" = o.id
WHERE o.id IS NULL;

-- Reservation items without valid reservations
SELECT ri.id, ri."reservationId"
FROM restaurante.reservation_items ri
LEFT JOIN restaurante.reservations r ON ri."reservationId" = r.id
WHERE r.id IS NULL;

-- Orders referencing deleted menu items
SELECT o.id, oi."menuItemId"
FROM restaurante.orders o
JOIN restaurante.order_items oi ON o.id = oi."orderId"
LEFT JOIN restaurante.menu_items mi ON oi."menuItemId" = mi.id
WHERE mi.id IS NULL;

-- Reservations with invalid table_ids
SELECT
  r.id,
  r.table_ids,
  r.table_ids - ARRAY(SELECT id FROM restaurante.tables) as invalid_tables
FROM restaurante.reservations r
WHERE NOT (r.table_ids <@ ARRAY(SELECT id FROM restaurante.tables));
```

#### Stock Reconciliation
```sql
-- /scripts/integrity/stock-reconciliation.sql
-- Verify stock calculations match order history

WITH order_quantities AS (
  SELECT
    oi."menuItemId",
    SUM(oi.quantity) as total_ordered
  FROM restaurante.order_items oi
  JOIN restaurante.orders o ON oi."orderId" = o.id
  WHERE o.status IN ('PENDING', 'CONFIRMED', 'IN_PROGRESS', 'READY', 'SERVED')
  GROUP BY oi."menuItemId"
)
SELECT
  mi.id,
  mi.name,
  mi.stock as current_stock,
  COALESCE(oq.total_ordered, 0) as total_ordered,
  100 as assumed_initial_stock,  -- Replace with actual initial stock
  100 - COALESCE(oq.total_ordered, 0) as expected_stock,
  mi.stock - (100 - COALESCE(oq.total_ordered, 0)) as discrepancy
FROM restaurante.menu_items mi
LEFT JOIN order_quantities oq ON mi.id = oq."menuItemId"
WHERE mi.stock != (100 - COALESCE(oq.total_ordered, 0))
ORDER BY ABS(mi.stock - (100 - COALESCE(oq.total_ordered, 0))) DESC;
```

### 3.3 Log Analysis Patterns

#### Email Delivery Tracking
```sql
-- /scripts/monitoring/email-logs.sql
SELECT
  el.sent_at,
  el.email_type,
  el.status,
  el.recipient_email,
  o."orderNumber",
  el.error_message,
  el.provider_message_id
FROM restaurante.email_logs el
LEFT JOIN restaurante.orders o ON el.order_id = o.id
WHERE el.sent_at > now() - interval '24 hours'
ORDER BY el.sent_at DESC;

-- Failed emails requiring retry
SELECT
  el.id,
  el.recipient_email,
  el.email_type,
  el.error_message,
  el.retry_count
FROM restaurante.email_logs el
WHERE el.status = 'failed'
  AND el.retry_count < 3
  AND el.sent_at > now() - interval '48 hours'
ORDER BY el.sent_at;
```

#### QR Scan Activity
```sql
-- /scripts/monitoring/qr-activity.sql
-- Detect suspicious scan patterns (potential fraud)
SELECT
  qs.table_id,
  t.number as table_number,
  count(*) as scan_count,
  count(DISTINCT qs.client_ip) as unique_ips,
  min(qs.scanned_at) as first_scan,
  max(qs.scanned_at) as last_scan
FROM restaurante.qr_scans qs
JOIN restaurante.tables t ON qs.table_id = t.id
WHERE qs.scanned_at > now() - interval '1 hour'
GROUP BY qs.table_id, t.number
HAVING count(*) > 10  -- More than 10 scans per hour
ORDER BY scan_count DESC;
```

---

## 4. Monitoring & Alerts

### 4.1 Critical Metrics Dashboard

**Setup:** Grafana + PostgreSQL Exporter

#### Metrics to Track

| Metric | Query | Alert Threshold | Priority |
|--------|-------|-----------------|----------|
| Connection Pool Utilization | `SELECT count(*) * 100.0 / (SELECT setting::int FROM pg_settings WHERE name='max_connections') FROM pg_stat_activity WHERE datname='postgres'` | >80% | üî¥ CRITICAL |
| Average Query Duration | `SELECT avg(mean_exec_time) FROM pg_stat_statements` | >500ms | ‚ö†Ô∏è WARNING |
| Dead Tuple Ratio | `SELECT sum(n_dead_tup)*100.0/NULLIF(sum(n_live_tup+n_dead_tup),0) FROM pg_stat_user_tables WHERE schemaname='restaurante'` | >20% | üü° INFO |
| Replication Lag | `SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()))` | >60s | ‚ö†Ô∏è WARNING |
| Cache Hit Ratio | `SELECT sum(blks_hit)*100.0/NULLIF(sum(blks_hit+blks_read),0) FROM pg_stat_database WHERE datname='postgres'` | <90% | üü° INFO |
| Failed Transactions | `SELECT xact_rollback FROM pg_stat_database WHERE datname='postgres'` | >100/min | ‚ö†Ô∏è WARNING |

### 4.2 Alerting Rules

#### PagerDuty / Slack Integration
```bash
# /scripts/alerts/check-db-health.sh
#!/bin/bash

DB_HOST="31.97.182.226"
DB_USER="postgres"
DB_NAME="postgres"
WEBHOOK_URL="https://hooks.slack.com/services/YOUR/WEBHOOK/URL"

# Check connection pool
CONN_PCT=$(ssh root@$DB_HOST "docker exec -i \$(docker ps -qf 'name=db') psql -U $DB_USER -d $DB_NAME -t -c \"SELECT ROUND(count(*)*100.0/(SELECT setting::int FROM pg_settings WHERE name='max_connections'),2) FROM pg_stat_activity WHERE datname='postgres'\"")

if (( $(echo "$CONN_PCT > 80" | bc -l) )); then
  curl -X POST $WEBHOOK_URL -d "{\"text\":\"üî¥ CRITICAL: DB connection pool at ${CONN_PCT}%\"}"
fi

# Check for long-running queries
LONG_QUERIES=$(ssh root@$DB_HOST "docker exec -i \$(docker ps -qf 'name=db') psql -U $DB_USER -d $DB_NAME -t -c \"SELECT count(*) FROM pg_stat_activity WHERE state != 'idle' AND query_start < now() - interval '30 seconds'\"")

if [ "$LONG_QUERIES" -gt 5 ]; then
  curl -X POST $WEBHOOK_URL -d "{\"text\":\"‚ö†Ô∏è WARNING: ${LONG_QUERIES} queries running >30s\"}"
fi
```

**Cron Schedule:**
```cron
# /etc/cron.d/db-health-checks
*/5 * * * * /scripts/alerts/check-db-health.sh
0 */4 * * * /scripts/monitoring/slow-queries.sql | mail -s "Slow Query Report" devops@enigmaconalma.com
0 2 * * * /scripts/integrity/orphaned-records.sql | mail -s "Data Integrity Check" devops@enigmaconalma.com
```

### 4.3 Backup Verification

```sql
-- /scripts/monitoring/backup-status.sql
-- Verify last backup timestamp (assuming WAL archiving)
SELECT
  pg_is_in_recovery() as is_replica,
  pg_last_wal_replay_lsn() as replay_lsn,
  pg_current_wal_lsn() as current_lsn,
  EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag_seconds;

-- Check backup destination (requires external monitoring)
-- Expected: Daily pg_dump + WAL archiving to S3/B2
```

**Backup Strategy Recommendation:**
```bash
#!/bin/bash
# /scripts/backup/daily-backup.sh

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backups/postgres"
DB_HOST="31.97.182.226"

# Full database dump
ssh root@$DB_HOST "docker exec \$(docker ps -qf 'name=db') pg_dump -U postgres -d postgres --schema=restaurante -F c" > "$BACKUP_DIR/restaurante_$TIMESTAMP.dump"

# Verify backup integrity
pg_restore --list "$BACKUP_DIR/restaurante_$TIMESTAMP.dump" > /dev/null
if [ $? -eq 0 ]; then
  echo "‚úÖ Backup verified: restaurante_$TIMESTAMP.dump"
else
  echo "üî¥ Backup FAILED verification"
  exit 1
fi

# Upload to remote storage (B2/S3)
# b2 upload-file enigma-backups "$BACKUP_DIR/restaurante_$TIMESTAMP.dump" "db/restaurante_$TIMESTAMP.dump"

# Cleanup old backups (keep 30 days)
find "$BACKUP_DIR" -name "restaurante_*.dump" -mtime +30 -delete
```

---

## 5. Testing Strategy

### 5.1 Integration Tests for Critical Flows

#### Order Creation with Stock Management
```typescript
// /tests/integration/order-with-stock.test.ts
import { describe, it, expect, beforeEach } from 'vitest'
import { createDirectAdminRestauranteClient } from '@/lib/supabase/server'

describe('Order Creation with Stock Management', () => {
  let supabase: any
  let testMenuItemId: string
  let initialStock: number

  beforeEach(async () => {
    supabase = createDirectAdminRestauranteClient()

    // Setup: Create test menu item with known stock
    const { data: menuItem } = await supabase
      .from('menu_items')
      .insert({
        id: `test_item_${Date.now()}`,
        name: 'Test Item',
        description: 'Test',
        price: 10.00,
        stock: 100,
        isAvailable: true,
        categoryId: 'test_category',
        restaurantId: 'rest_enigma_001'
      })
      .select()
      .single()

    testMenuItemId = menuItem.id
    initialStock = menuItem.stock
  })

  it('should decrease stock atomically on order creation', async () => {
    const orderQuantity = 5

    // Create order
    const { data: order } = await supabase
      .from('orders')
      .insert({
        id: `test_order_${Date.now()}`,
        orderNumber: `TEST${Date.now()}`,
        totalAmount: 50.00,
        status: 'PENDING',
        tableId: 'test_table',
        restaurantId: 'rest_enigma_001',
        orderedAt: new Date(),
        updatedAt: new Date()
      })
      .select()
      .single()

    // Create order item
    await supabase
      .from('order_items')
      .insert({
        id: `test_item_${Date.now()}`,
        orderId: order.id,
        menuItemId: testMenuItemId,
        quantity: orderQuantity,
        unitPrice: 10.00,
        totalPrice: 50.00,
        status: 'PENDING',
        createdAt: new Date(),
        updatedAt: new Date()
      })

    // Decrease stock using RPC
    await supabase.rpc('decrease_menu_item_stock', {
      item_id: testMenuItemId,
      decrease_amount: orderQuantity
    })

    // Verify stock decreased correctly
    const { data: updatedItem } = await supabase
      .from('menu_items')
      .select('stock')
      .eq('id', testMenuItemId)
      .single()

    expect(updatedItem.stock).toBe(initialStock - orderQuantity)
  })

  it('should rollback on insufficient stock', async () => {
    const orderQuantity = 150  // More than available stock

    // Attempt to decrease stock beyond availability
    const { error } = await supabase.rpc('decrease_menu_item_stock', {
      item_id: testMenuItemId,
      decrease_amount: orderQuantity
    })

    expect(error).toBeTruthy()
    expect(error.message).toContain('Stock insuficiente')

    // Verify stock unchanged
    const { data: item } = await supabase
      .from('menu_items')
      .select('stock')
      .eq('id', testMenuItemId)
      .single()

    expect(item.stock).toBe(initialStock)
  })
})
```

### 5.2 Stress Testing for Stock Concurrency

```typescript
// /tests/stress/concurrent-stock-updates.test.ts
import { describe, it } from 'vitest'

describe('Concurrent Stock Updates', () => {
  it('should handle 50 concurrent orders without race conditions', async () => {
    const menuItemId = 'test_item_concurrent'
    const initialStock = 100
    const concurrentOrders = 50
    const quantityPerOrder = 1

    // Setup
    await setupMenuItem(menuItemId, initialStock)

    // Execute 50 concurrent orders
    const promises = Array.from({ length: concurrentOrders }, (_, i) =>
      createOrderWithStock(menuItemId, quantityPerOrder, i)
    )

    const results = await Promise.allSettled(promises)
    const successful = results.filter(r => r.status === 'fulfilled').length
    const failed = results.filter(r => r.status === 'rejected').length

    console.log(`‚úÖ Successful orders: ${successful}`)
    console.log(`‚ùå Failed orders: ${failed}`)

    // Verify final stock is consistent
    const finalStock = await getMenuItemStock(menuItemId)
    expect(finalStock).toBe(initialStock - successful)
  })
})
```

**Load Testing with Artillery:**
```yaml
# /tests/load/order-creation.yml
config:
  target: "https://enigmaconalma.com"
  phases:
    - duration: 60
      arrivalRate: 5  # 5 orders per second
      name: "Sustained load"
    - duration: 30
      arrivalRate: 20  # Spike to 20 orders/sec
      name: "Peak load"
  processor: "./order-processor.js"

scenarios:
  - name: "Create order with stock decrease"
    flow:
      - post:
          url: "/api/orders"
          json:
            orderNumber: "LOAD_{{ $randomString() }}"
            tableId: "mesa_1"
            items:
              - menuItemId: "test_item_stress"
                quantity: 1
                unitPrice: 10.00
          capture:
            - json: "$.order.id"
              as: "orderId"
      - think: 2
      - get:
          url: "/api/orders/{{ orderId }}"
```

### 5.3 Failure Scenario Simulation

```sql
-- /tests/chaos/simulate-failures.sql

-- 1. Simulate connection pool exhaustion
-- Open 100 connections and leave them idle
DO $$
BEGIN
  FOR i IN 1..100 LOOP
    PERFORM pg_sleep(0.1);
  END LOOP;
END $$;

-- 2. Simulate long-running transaction blocking orders table
BEGIN;
SELECT * FROM restaurante.orders WHERE id = 'test_order' FOR UPDATE;
-- Keep transaction open for 30 seconds
SELECT pg_sleep(30);
COMMIT;

-- 3. Simulate stock depletion edge case
UPDATE restaurante.menu_items
SET stock = 1
WHERE id = 'test_item';

-- Then attempt 5 concurrent orders (4 should fail gracefully)

-- 4. Simulate foreign key violation
-- Attempt order creation with non-existent tableId
INSERT INTO restaurante.orders (
  id, "orderNumber", "tableId", "totalAmount", status, "restaurantId", "orderedAt", "updatedAt"
)
VALUES (
  'test_fk_violation',
  'INVALID001',
  'non_existent_table',  -- Should fail with FK constraint
  10.00,
  'PENDING',
  'rest_enigma_001',
  NOW(),
  NOW()
);
```

---

## 6. Performance Optimization Recommendations

### 6.1 Missing Indexes (High Priority)

```sql
-- /migrations/add-performance-indexes.sql

-- 1. Order lookups by customer email (for token-based tracking)
CREATE INDEX CONCURRENTLY idx_orders_customer_email_status
ON restaurante.orders(customer_email, status)
WHERE customer_email IS NOT NULL;

-- 2. Order items by order status (for kitchen dashboard)
CREATE INDEX CONCURRENTLY idx_order_items_status_created
ON restaurante.order_items(status, "createdAt");

-- 3. Reservations by date range queries (most common filter)
CREATE INDEX CONCURRENTLY idx_reservations_date_status
ON restaurante.reservations(date, status);

-- 4. QR scans by table and timestamp (fraud detection)
CREATE INDEX CONCURRENTLY idx_qr_scans_table_timestamp
ON restaurante.qr_scans(table_id, scanned_at DESC);

-- 5. Email logs by status for retry logic
CREATE INDEX CONCURRENTLY idx_email_logs_status_retry
ON restaurante.email_logs(status, retry_count)
WHERE status = 'failed';

-- 6. Composite index for table availability checks
CREATE INDEX CONCURRENTLY idx_reservations_table_time
ON restaurante.reservations(table_ids, date, time)
WHERE status IN ('PENDING', 'CONFIRMED', 'SEATED');
```

**Expected Performance Improvements:**
- Customer order history queries: **5x faster** (500ms ‚Üí 100ms)
- Table availability checks: **3x faster** (300ms ‚Üí 100ms)
- Email retry queue: **10x faster** (sequential scan ‚Üí index scan)

### 6.2 PostgreSQL Configuration Tuning

```conf
# /etc/postgresql/postgresql.conf
# Recommended settings for 32GB VPS with 8 CPUs

# MEMORY SETTINGS (Current: 128MB shared_buffers - TOO LOW)
shared_buffers = 8GB              # 25% of total RAM (currently 128MB)
effective_cache_size = 24GB       # 75% of total RAM (currently 128MB)
maintenance_work_mem = 2GB        # For VACUUM, CREATE INDEX (currently 64MB)
work_mem = 64MB                   # Per-operation memory (currently 4MB)

# CHECKPOINT SETTINGS
checkpoint_completion_target = 0.9  # Currently 0.5 (causes I/O spikes)
wal_buffers = 16MB                  # Currently 512kB
checkpoint_timeout = 15min          # Default 5min

# QUERY PLANNER
default_statistics_target = 500     # Currently 100 (improves query plans)
random_page_cost = 1.1              # For SSD storage (default 4.0)

# CONNECTION POOLING
max_connections = 200               # Currently 100 (safe increase)

# LOGGING (Production monitoring)
log_min_duration_statement = 1000   # Log queries >1s
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on
log_temp_files = 0

# AUTOVACUUM TUNING
autovacuum_max_workers = 4          # More aggressive cleanup
autovacuum_naptime = 30s            # Check more frequently
autovacuum_vacuum_threshold = 50
autovacuum_analyze_threshold = 50
```

**Apply Configuration:**
```bash
# Restart required for most settings
ssh root@31.97.182.226 "docker restart \$(docker ps -qf 'name=db')"

# Verify settings
ssh root@31.97.182.226 "docker exec -i \$(docker ps -qf 'name=db') psql -U postgres -c 'SHOW shared_buffers;'"
```

### 6.3 Connection Pooling Configuration

**Current Setup:** Next.js API routes use `@supabase/ssr` with no explicit pooling.

**Recommended:** Add PgBouncer between application and database.

```yaml
# /docker-compose.yml (add to existing setup)
services:
  pgbouncer:
    image: pgbouncer/pgbouncer:latest
    environment:
      - DATABASES_HOST=supabase-db
      - DATABASES_PORT=5432
      - DATABASES_USER=postgres
      - DATABASES_PASSWORD=EncryptionConAlma23
      - DATABASES_DBNAME=postgres
      - PGBOUNCER_POOL_MODE=transaction  # Most aggressive
      - PGBOUNCER_MAX_CLIENT_CONN=500
      - PGBOUNCER_DEFAULT_POOL_SIZE=25
      - PGBOUNCER_MIN_POOL_SIZE=10
      - PGBOUNCER_RESERVE_POOL_SIZE=5
      - PGBOUNCER_MAX_DB_CONNECTIONS=100
    ports:
      - "6432:6432"
    depends_on:
      - supabase-db
```

**Connection String Update:**
```env
# .env.local
DATABASE_URL="postgresql://postgres:EncryptionConAlma23@31.97.182.226:6432/postgres"
# Use port 6432 (PgBouncer) instead of 5432 (direct PostgreSQL)
```

**Benefits:**
- Reduced connection overhead (reuse pooled connections)
- Handle 500 concurrent clients with only 25-50 DB connections
- Automatic connection recycling
- Query timeout enforcement

---

## 7. Data Migration Testing

### 7.1 Migration Rollback Procedures

```sql
-- /migrations/rollback-template.sql
-- ALWAYS create rollback scripts before applying migrations

-- Example: Rollback adding idempotency_key column
BEGIN;

-- Save existing data if needed
CREATE TABLE IF NOT EXISTS restaurante._migration_backup_orders_20251001 AS
SELECT * FROM restaurante.orders;

-- Drop new column
ALTER TABLE restaurante.orders
DROP COLUMN IF EXISTS idempotency_key;

-- Drop new index
DROP INDEX IF EXISTS restaurante.idx_orders_idempotency_key;

-- Verify rollback
SELECT
  column_name
FROM information_schema.columns
WHERE table_schema = 'restaurante'
  AND table_name = 'orders'
  AND column_name = 'idempotency_key';
-- Should return 0 rows

COMMIT;  -- Only commit if verification passes
```

### 7.2 Zero-Downtime Migration Strategy

**Pattern: Expand-Migrate-Contract**

```sql
-- PHASE 1: EXPAND (Add new column, no writes yet)
ALTER TABLE restaurante.orders
ADD COLUMN new_status_v2 VARCHAR(50);

-- Create index in background
CREATE INDEX CONCURRENTLY idx_orders_new_status
ON restaurante.orders(new_status_v2);

-- PHASE 2: MIGRATE (Dual-write application code)
-- Application writes to BOTH status and new_status_v2
-- Deploy application update here (no downtime)

-- Backfill existing data
UPDATE restaurante.orders
SET new_status_v2 = status::text
WHERE new_status_v2 IS NULL;

-- PHASE 3: CONTRACT (Remove old column)
-- Application now only reads/writes new_status_v2

ALTER TABLE restaurante.orders
DROP COLUMN status;

-- Rename new column to final name
ALTER TABLE restaurante.orders
RENAME COLUMN new_status_v2 TO status;
```

---

## 8. Production Deployment Checklist

### Pre-Deployment
- [ ] Run `EXPLAIN ANALYZE` on all new queries
- [ ] Test migration on staging with production-like data volume
- [ ] Verify rollback procedure works
- [ ] Check pg_stat_statements enabled
- [ ] Backup database before deployment
- [ ] Alert team of maintenance window (even if zero-downtime)

### During Deployment
- [ ] Monitor connection pool utilization
- [ ] Watch for slow query alerts
- [ ] Check error logs for constraint violations
- [ ] Verify autovacuum running on modified tables

### Post-Deployment
- [ ] Run integrity checks (orphaned records script)
- [ ] Verify stock reconciliation
- [ ] Check email delivery logs
- [ ] Monitor cache hit ratio
- [ ] Review query performance in pg_stat_statements

---

## 9. Incident Response Runbook

### Emergency Contacts
- **Database Admin:** devops@enigmaconalma.com
- **VPS Provider:** Hetzner Support (31.97.182.226)
- **Application Team:** dev@enigmaconalma.com

### Incident Severity Levels

#### üî¥ P0 - CRITICAL (Database Down)
**Symptoms:** Connection refused, all queries failing

**Response:**
```bash
# 1. Check Docker container status
ssh root@31.97.182.226 "docker ps -a | grep db"

# 2. Check logs
ssh root@31.97.182.226 "docker logs \$(docker ps -qf 'name=db') --tail 100"

# 3. Restart container if crashed
ssh root@31.97.182.226 "docker restart \$(docker ps -aqf 'name=db')"

# 4. Verify recovery
ssh root@31.97.182.226 "docker exec -i \$(docker ps -qf 'name=db') psql -U postgres -c 'SELECT 1'"
```

**Escalation:** If restart fails, restore from latest backup.

#### ‚ö†Ô∏è P1 - HIGH (Performance Degradation)
**Symptoms:** Slow queries, high connection count, timeouts

**Response:**
```sql
-- 1. Identify slow queries
SELECT pid, query_start, state, left(query, 100)
FROM pg_stat_activity
WHERE state != 'idle' AND query_start < now() - interval '10 seconds'
ORDER BY query_start;

-- 2. Terminate long-running queries (CAUTION)
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE state != 'idle'
  AND query_start < now() - interval '5 minutes'
  AND pid != pg_backend_pid();

-- 3. Check for lock contention
SELECT * FROM pg_locks WHERE NOT granted;

-- 4. Emergency vacuum if high bloat
VACUUM (VERBOSE, ANALYZE) restaurante.orders;
```

#### üü° P2 - MEDIUM (Data Inconsistency)
**Symptoms:** Stock mismatches, orphaned records

**Response:**
```bash
# Run integrity checks
/scripts/integrity/orphaned-records.sql
/scripts/integrity/stock-reconciliation.sql

# Manual data correction (requires audit)
# Document all changes in incident log
```

---

## 10. Summary of Recommended Actions

### Immediate (Week 1)
1. ‚úÖ **Enable pg_stat_statements** for query monitoring
2. ‚úÖ **Add missing indexes** (orders, reservations, email_logs)
3. ‚úÖ **Run VACUUM on orders table** (32 dead tuples)
4. ‚úÖ **Create connection pool monitoring script** (cron every 5 min)
5. ‚úÖ **Implement idempotency keys** for order creation

### Short-term (Month 1)
6. ‚úÖ **Tune PostgreSQL memory settings** (shared_buffers to 8GB)
7. ‚úÖ **Deploy PgBouncer** for connection pooling
8. ‚úÖ **Add database-level constraints** (price, stock validation)
9. ‚úÖ **Setup Grafana dashboard** for metrics visualization
10. ‚úÖ **Implement backup verification script** (daily automated)

### Medium-term (Quarter 1)
11. ‚úÖ **Write integration tests** for critical flows
12. ‚úÖ **Conduct load testing** with Artillery (20 orders/sec target)
13. ‚úÖ **Create incident response documentation**
14. ‚úÖ **Implement automated alerting** (Slack/PagerDuty)
15. ‚úÖ **Optimize slow queries** identified in pg_stat_statements

---

## Appendix A: Quick Reference Commands

```bash
# CONNECTION HEALTH
ssh root@31.97.182.226 "docker exec -i \$(docker ps -qf 'name=db') psql -U postgres -c 'SELECT count(*) FROM pg_stat_activity WHERE datname='\''postgres'\'';'"

# VACUUM CRITICAL TABLES
ssh root@31.97.182.226 "docker exec -i \$(docker ps -qf 'name=db') psql -U postgres -c 'VACUUM ANALYZE restaurante.orders;'"

# BACKUP DATABASE
ssh root@31.97.182.226 "docker exec \$(docker ps -qf 'name=db') pg_dump -U postgres -d postgres --schema=restaurante -F c" > backup_$(date +%Y%m%d).dump

# RESTORE DATABASE
cat backup_20251001.dump | ssh root@31.97.182.226 "docker exec -i \$(docker ps -qf 'name=db') pg_restore -U postgres -d postgres --schema=restaurante"

# CHECK TABLE SIZES
ssh root@31.97.182.226 "docker exec -i \$(docker ps -qf 'name=db') psql -U postgres -c \"SELECT schemaname, relname, pg_size_pretty(pg_total_relation_size(relid)) FROM pg_stat_user_tables WHERE schemaname='restaurante' ORDER BY pg_total_relation_size(relid) DESC;\""
```

---

**Document End**

For questions or updates, contact: devops@enigmaconalma.com
